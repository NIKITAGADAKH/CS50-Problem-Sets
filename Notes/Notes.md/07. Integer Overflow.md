# 1. Integer Overflow
Integer overflow happens when an integer can only be so large given a finite number of bits.

The Y2K problem arose because many programs stored the calendar year with just two digits, like `98` for 1998, and `99` for 1999. When the year 2000 approached, the programs had to store only `00`, leading to confusion betwenn the years 1900 and 2000.

In 2038, we'll also run out of bits to track time, since the standard number of bits to count the number of seconds since January 1st 1970 is 32. A 32 bit integer can only count up to about two billion, so in 2038 we'll reach that limit.

* the 32 bit of an integer representing 2147483647 looks like:

``` 01111111111111111111111111111111```

* when we increase that by 1, the bits will look like:

``` 10000000000000000000000000000000```

* but the first bit in an integer represent whether or not it's a negative value, so the decimal value will actually be -2147483647, the lowest possible negative value of an int. So computers might actually think it's sometime in 1901.

In Python, integers have *arbitrary precision*, meaning that arbitrarily large numbers can be represented, the only limitation being memory. Integer overflow don't normally occur if the operations are purely performed in **Python.**


